{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP90OxoLYgjp4y6HFKuxt1Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EduardoRosa25/Trabalho_pratico_01_Grupo01/blob/main/Trabalho_Pratico_01_Grupo1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Alunos:\n",
        "#  Bernardo Carvalho Trindade 12321BSI253\n",
        "#  Eduardo Antonio 12311BSI317\n",
        "#  Eduardo Rosa 12311BSI275\n",
        "#  Luiz Fellipe 12311BSI262\n",
        "#  Lucas Pinheiro Barbosa 12221BSI224"
      ],
      "metadata": {
        "id": "k7sA57U5hnOf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nr5Oy01WaBW1",
        "outputId": "19239895-53ac-4bd5-e13e-df9b6a930418"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Ambiente configurado e bibliotecas importadas!\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas numpy nltk\n",
        "\n",
        "import json\n",
        "import re\n",
        "import sys\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import RSLPStemmer\n",
        "from typing import List, Dict, Tuple, Set, Any\n",
        "from collections import defaultdict\n",
        "import zipfile\n",
        "\n",
        "print(\"Ambiente configurado e bibliotecas importadas!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- CLASSE 1: DOCUMENTO ---\n",
        "class Documento:\n",
        "    def __init__(self, doc_id: str, texto_original: str):\n",
        "        self.id = doc_id\n",
        "        self.texto_original = texto_original\n",
        "        self.termos_processados: List[str] = []\n",
        "\n",
        "# --- CLASSE 2: PROCESSADOR DE TEXTO ---\n",
        "class ProcessadorTexto:\n",
        "    def __init__(self):\n",
        "        self._configurar_nltk()\n",
        "        self.stemmer = RSLPStemmer()\n",
        "        self.stopwords_pt = set(stopwords.words('portuguese'))\n",
        "\n",
        "    def _configurar_nltk(self):\n",
        "        try:\n",
        "            nltk.data.find('corpora/stopwords')\n",
        "            nltk.data.find('stemmers/rslp')\n",
        "        except LookupError:\n",
        "            print(\"[DEBUG] Baixando recursos do NLTK...\")\n",
        "            nltk.download('stopwords')\n",
        "            nltk.download('rslp')\n",
        "\n",
        "    def processar(self, texto: str) -> List[str]:\n",
        "        texto = texto.lower()\n",
        "        texto = re.sub(r'[^a-zà-úÀ-Ú\\s]', '', texto)\n",
        "        tokens = texto.split()\n",
        "\n",
        "        tokens_processados = [\n",
        "            self.stemmer.stem(t)\n",
        "            for t in tokens\n",
        "            if t not in self.stopwords_pt and len(t) > 1\n",
        "        ]\n",
        "        return tokens_processados\n",
        "\n",
        "# --- CLASSE 3: ÍNDICE INVERTIDO (Com Busca por Frase) ---\n",
        "class IndiceInvertido:\n",
        "    def __init__(self):\n",
        "        self.indice: Dict[str, Dict[str, List[int]]] = {}\n",
        "\n",
        "    def construir_indice(self, documentos: Dict[str, Documento]):\n",
        "        self.indice.clear()\n",
        "        for doc_id, doc in documentos.items():\n",
        "            for posicao, termo in enumerate(doc.termos_processados):\n",
        "                if termo not in self.indice:\n",
        "                    self.indice[termo] = {}\n",
        "                if doc_id not in self.indice[termo]:\n",
        "                    self.indice[termo][doc_id] = []\n",
        "                self.indice[termo][doc_id].append(posicao)\n",
        "\n",
        "    def get_indice(self) -> Dict[str, Dict[str, List[int]]]:\n",
        "        return self.indice\n",
        "\n",
        "    def exibir_indice(self):\n",
        "        if not self.indice:\n",
        "            print(\"[INFO] O Índice Invertido está vazio.\")\n",
        "            return\n",
        "        print(\"\\n--- Índice Invertido (Termo: {Doc_ID: [posições]}) ---\")\n",
        "        termos_ordenados = sorted(self.indice.keys())\n",
        "        for termo in termos_ordenados:\n",
        "            posting = self.indice[termo]\n",
        "            formatado = \" | \".join([f\"{did}: {posting[did]}\" for did in sorted(posting.keys())])\n",
        "            print(f\"'{termo}': {formatado}\")\n",
        "\n",
        "    def buscar_por_frase(self, frase_processada: List[str]) -> Set[str]:\n",
        "        if not frase_processada:\n",
        "            return set()\n",
        "\n",
        "        termo_inicial = frase_processada[0]\n",
        "        if termo_inicial not in self.indice:\n",
        "            return set()\n",
        "\n",
        "        candidatos = set()\n",
        "        docs_do_termo_inicial = self.indice[termo_inicial]\n",
        "\n",
        "        for doc_id, posicoes_iniciais in docs_do_termo_inicial.items():\n",
        "            for pos_inicio in posicoes_iniciais:\n",
        "                match_completo = True\n",
        "                for i in range(1, len(frase_processada)):\n",
        "                    termo_seguinte = frase_processada[i]\n",
        "                    posicao_esperada = pos_inicio + i\n",
        "\n",
        "                    if (termo_seguinte not in self.indice or\n",
        "                        doc_id not in self.indice[termo_seguinte] or\n",
        "                        posicao_esperada not in self.indice[termo_seguinte][doc_id]):\n",
        "                        match_completo = False\n",
        "                        break\n",
        "\n",
        "                if match_completo:\n",
        "                    candidatos.add(doc_id)\n",
        "                    break\n",
        "        return candidatos\n",
        "\n",
        "# --- CLASSE 4: MATRIZ TF-IDF (Com Booleano e Vetorial) ---\n",
        "class MatrizTFIDF:\n",
        "    def __init__(self):\n",
        "        self.vocabulario: List[str] = []\n",
        "        self.matriz_tf: pd.DataFrame = pd.DataFrame()\n",
        "        self.vetor_idf: pd.Series = pd.Series()\n",
        "        self.matriz_tfidf: pd.DataFrame = pd.DataFrame()\n",
        "        self.normas_doc: Dict[str, float] = {}\n",
        "        self.processador = ProcessadorTexto()\n",
        "\n",
        "    def construir_matrizes(self, documentos: Dict[str, Documento]):\n",
        "        if not documentos:\n",
        "            return\n",
        "\n",
        "        nomes_docs = list(documentos.keys())\n",
        "        docs_processados = [doc.termos_processados for doc in documentos.values()]\n",
        "\n",
        "        self.vocabulario = sorted(set([t for doc in docs_processados for t in doc]))\n",
        "\n",
        "        self.matriz_tf = pd.DataFrame(0.0, index=self.vocabulario, columns=nomes_docs)\n",
        "        for nome, termos in zip(nomes_docs, docs_processados):\n",
        "            for termo in termos:\n",
        "                self.matriz_tf.loc[termo, nome] += 1\n",
        "\n",
        "        self.matriz_tf = self.matriz_tf.map(lambda x: 1 + np.log2(x) if x > 0 else 0)\n",
        "\n",
        "        N = len(nomes_docs)\n",
        "        ni = (self.matriz_tf > 0).sum(axis=1)\n",
        "        idf_vals = np.log2(N / ni.replace(0, 1))\n",
        "        self.vetor_idf = pd.Series(idf_vals, index=self.vocabulario)\n",
        "\n",
        "        self.matriz_tfidf = self.matriz_tf.mul(self.vetor_idf, axis=0)\n",
        "        self.normas_doc = np.sqrt((self.matriz_tfidf**2).sum(axis=0)).to_dict()\n",
        "\n",
        "    def exibir_matriz_tfidf(self):\n",
        "        if self.matriz_tfidf.empty:\n",
        "            print(\"[INFO] Matriz vazia.\")\n",
        "        else:\n",
        "            print(self.matriz_tfidf.round(4))\n",
        "\n",
        "    def buscar_booleana(self, query: str, operador: str) -> List[str]:\n",
        "        if self.matriz_tfidf.empty: return []\n",
        "\n",
        "        termos_query = self.processador.processar(query)\n",
        "        termos_validos = [t for t in termos_query if t in self.matriz_tfidf.index]\n",
        "        if not termos_validos: return []\n",
        "\n",
        "        presenca = (self.matriz_tfidf.loc[termos_validos] > 0).astype(int)\n",
        "        operador = operador.upper()\n",
        "\n",
        "        if operador == 'AND':\n",
        "            match = presenca.sum(axis=0) == len(termos_validos)\n",
        "            return match[match].index.tolist()\n",
        "        elif operador == 'OR':\n",
        "            match = presenca.sum(axis=0) > 0\n",
        "            return match[match].index.tolist()\n",
        "        elif operador == 'NOT':\n",
        "            match_pos = presenca.sum(axis=0) > 0\n",
        "            docs_com_termo = match_pos[match_pos].index.tolist()\n",
        "            todos = self.matriz_tfidf.columns.tolist()\n",
        "            return list(set(todos) - set(docs_com_termo))\n",
        "        return []\n",
        "\n",
        "    def buscar_similaridade(self, query: str, indice: Dict) -> List[Tuple[str, float]]:\n",
        "        if self.matriz_tfidf.empty: return []\n",
        "\n",
        "        q_termos = self.processador.processar(query)\n",
        "        q_tf = pd.Series([q_termos.count(t) for t in self.vocabulario], index=self.vocabulario)\n",
        "        q_tf = q_tf.map(lambda x: 1 + np.log2(x) if x > 0 else 0)\n",
        "\n",
        "        q_vec = q_tf * self.vetor_idf\n",
        "        q_norm = np.sqrt((q_vec**2).sum())\n",
        "        if q_norm == 0: return []\n",
        "\n",
        "        candidatos = set()\n",
        "        for t in q_termos:\n",
        "            if t in indice:\n",
        "                candidatos.update(indice[t].keys())\n",
        "\n",
        "        resultados = []\n",
        "        for doc_id in candidatos:\n",
        "            doc_vec = self.matriz_tfidf[doc_id]\n",
        "            dot = (q_vec * doc_vec).sum()\n",
        "            d_norm = self.normas_doc.get(doc_id, 0)\n",
        "\n",
        "            if d_norm > 0:\n",
        "                score = dot / (q_norm * d_norm)\n",
        "                if score > 0:\n",
        "                    resultados.append((doc_id, score))\n",
        "\n",
        "        return sorted(resultados, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# --- CLASSE 5: COLEÇÃO (O Maestro) ---\n",
        "class Colecao:\n",
        "    def __init__(self):\n",
        "        self.documentos: Dict[str, Documento] = {}\n",
        "        self.vocabulario: List[str] = []\n",
        "        self.fila_documentos: List[Dict[str, str]] = []\n",
        "        self.lista_completa_backup: List[Dict[str, str]] = []\n",
        "\n",
        "        # Instancia as classes que já definimos acima\n",
        "        self.matriz_handler = MatrizTFIDF()\n",
        "        self.processador = ProcessadorTexto()\n",
        "        self.indice_invertido_handler = IndiceInvertido()\n",
        "\n",
        "    def carregar_dados_iniciais(self, caminho_arquivo: str):\n",
        "        if self.lista_completa_backup: return\n",
        "\n",
        "        print(f\"[DEBUG] Lendo JSON: {caminho_arquivo}\")\n",
        "        try:\n",
        "            with open(caminho_arquivo, 'r', encoding='utf-8') as f:\n",
        "                dados = json.load(f)\n",
        "\n",
        "            dados_normalizados = []\n",
        "            if isinstance(dados, list):\n",
        "                dados_normalizados = dados\n",
        "            elif isinstance(dados, dict):\n",
        "                dados_normalizados = [{\"name\": k, \"content\": v} for k, v in dados.items()]\n",
        "\n",
        "            self.lista_completa_backup = list(dados_normalizados)\n",
        "            self.fila_documentos = list(dados_normalizados)\n",
        "            print(f\"[SUCESSO] {len(self.lista_completa_backup)} documentos carregados.\")\n",
        "\n",
        "        except FileNotFoundError:\n",
        "            print(f\"[ERRO] Arquivo '{caminho_arquivo}' não encontrado. Faça upload!\")\n",
        "        except Exception as e:\n",
        "            print(f\"[ERRO] Falha ao ler JSON: {e}\")\n",
        "\n",
        "    def adicionar_proximo_da_fila(self):\n",
        "        if not self.fila_documentos:\n",
        "            print(\"[AVISO] Fila vazia.\")\n",
        "            return\n",
        "\n",
        "        item = self.fila_documentos.pop(0)\n",
        "        d_id = item.get('name') or item.get('id')\n",
        "        d_conteudo = item.get('content') or item.get('text')\n",
        "\n",
        "        if d_id and d_conteudo:\n",
        "            if d_id not in self.documentos:\n",
        "                self._adicionar_documento_interno(d_id, d_conteudo)\n",
        "                print(f\"[SUCESSO] Documento {d_id} adicionado.\")\n",
        "            else:\n",
        "                print(f\"[INFO] Doc {d_id} já existe.\")\n",
        "\n",
        "    def adicionar_todos_restantes(self):\n",
        "        if not self.lista_completa_backup:\n",
        "            print(\"[ERRO] Carregue o JSON primeiro.\")\n",
        "            return\n",
        "\n",
        "        print(\"Processando inserção em lote...\")\n",
        "        count = 0\n",
        "        for item in self.lista_completa_backup:\n",
        "            d_id = item.get('name') or item.get('id')\n",
        "            d_conteudo = item.get('content') or item.get('text')\n",
        "\n",
        "            if d_id and d_conteudo and d_id not in self.documentos:\n",
        "                self._adicionar_documento_interno(d_id, d_conteudo, atualizar_agora=False)\n",
        "                count += 1\n",
        "\n",
        "        self.fila_documentos = []\n",
        "        if count > 0:\n",
        "            self._atualizar_sistema()\n",
        "            print(f\"[SUCESSO] {count} documentos adicionados.\")\n",
        "        else:\n",
        "            print(\"[INFO] Todos os documentos já estavam presentes.\")\n",
        "\n",
        "    def remover_documento(self, doc_id: str):\n",
        "        if doc_id in self.documentos:\n",
        "            del self.documentos[doc_id]\n",
        "            self._atualizar_sistema()\n",
        "            print(f\"[SUCESSO] Documento {doc_id} removido.\")\n",
        "        else:\n",
        "            print(\"[ERRO] ID não encontrado.\")\n",
        "\n",
        "    def _adicionar_documento_interno(self, doc_id: str, texto: str, atualizar_agora=True):\n",
        "        doc = Documento(doc_id, texto)\n",
        "        doc.termos_processados = self.processador.processar(texto)\n",
        "        self.documentos[doc_id] = doc\n",
        "\n",
        "        if atualizar_agora:\n",
        "            self._atualizar_sistema()\n",
        "\n",
        "    def _atualizar_sistema(self):\n",
        "        termos = set()\n",
        "        for doc in self.documentos.values():\n",
        "            termos.update(doc.termos_processados)\n",
        "        self.vocabulario = sorted(list(termos))\n",
        "\n",
        "        if self.documentos:\n",
        "            self.matriz_handler.construir_matrizes(self.documentos)\n",
        "            self.indice_invertido_handler.construir_indice(self.documentos)\n",
        "\n",
        "    def get_indice_invertido_handler(self): return self.indice_invertido_handler\n",
        "    def get_matriz_handler(self): return self.matriz_handler\n",
        "    def get_vocabulario(self): return self.vocabulario\n",
        "\n",
        "print(\"Todas as classes foram compiladas com sucesso!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJnKZyzUf120",
        "outputId": "53aadcf2-e7c3-47de-fb2a-02d1589e3d6f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Todas as classes foram compiladas com sucesso!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def print_menu():\n",
        "    print('\\n=== MENU PRINCIPAL ===')\n",
        "    print('1) Adicionar um documento por vez (fila)')\n",
        "    print('2) Adicionar TODOS os documentos (lote)')\n",
        "    print('3) Remover documento por ID')\n",
        "    print('4) Exibir Vocabulário')\n",
        "    print('5) Exibir Matriz TF-IDF')\n",
        "    print('6) Exibir Índice Invertido (Posicional)')\n",
        "    print('7) Consulta Booleana (AND, OR, NOT)')\n",
        "    print('8) Consulta por Similaridade (Ranking)')\n",
        "    print('9) Consulta por Frase (Exata)')\n",
        "    print('0) Sair')\n",
        "\n",
        "def main():\n",
        "    sistema = Colecao()\n",
        "    # Certifique-se de que este arquivo esteja na aba de arquivos do Colab\n",
        "    arquivo_json = 'colecao - trabalho 01.json'\n",
        "\n",
        "    sistema.carregar_dados_iniciais(arquivo_json)\n",
        "\n",
        "    while True:\n",
        "        print_menu()\n",
        "        choice = input('Escolha uma opção: ').strip()\n",
        "\n",
        "        try:\n",
        "            if choice == '1':\n",
        "                sistema.adicionar_proximo_da_fila()\n",
        "            elif choice == '2':\n",
        "                sistema.adicionar_todos_restantes()\n",
        "            elif choice == '3':\n",
        "                id_doc = input(\"Digite o ID do Documento (ex: D1): \").strip()\n",
        "                sistema.remover_documento(id_doc)\n",
        "            elif choice == '4':\n",
        "                vocab = sistema.get_vocabulario()\n",
        "                print(f\"\\nVocabulário ({len(vocab)} termos):\")\n",
        "                print(\", \".join(vocab))\n",
        "            elif choice == '5':\n",
        "                sistema.get_matriz_handler().exibir_matriz_tfidf()\n",
        "            elif choice == '6':\n",
        "                sistema.get_indice_invertido_handler().exibir_indice()\n",
        "            elif choice == '7':\n",
        "                if not sistema.documentos:\n",
        "                    print(\"[ERRO] Coleção vazia.\")\n",
        "                    continue\n",
        "                print(\"\\n--- Consulta Booleana ---\")\n",
        "                query = input(\"Termos da consulta: \")\n",
        "                operador = input(\"Operador (AND, OR, NOT): \").strip()\n",
        "                res = sistema.get_matriz_handler().buscar_booleana(query, operador)\n",
        "                if res:\n",
        "                    print(f\"✅ Documentos encontrados: {', '.join(res)}\")\n",
        "                else:\n",
        "                    print(f\"❌ Nenhum resultado para '{query}' ({operador}).\")\n",
        "            elif choice == '8':\n",
        "                if not sistema.documentos:\n",
        "                    print(\"[ERRO] Coleção vazia.\")\n",
        "                    continue\n",
        "                print(\"\\n--- Consulta por Similaridade (Cosseno) ---\")\n",
        "                query = input(\"Digite a consulta: \")\n",
        "                indice = sistema.get_indice_invertido_handler().get_indice()\n",
        "                ranking = sistema.get_matriz_handler().buscar_similaridade(query, indice)\n",
        "                if ranking:\n",
        "                    print(f\"✅ Ranking de Relevância:\")\n",
        "                    for doc_id, score in ranking:\n",
        "                        print(f\"1. {doc_id} (Score: {score:.4f})\")\n",
        "                else:\n",
        "                    print(\"❌ Nenhum documento relevante encontrado.\")\n",
        "            elif choice == '9':\n",
        "                if not sistema.documentos:\n",
        "                    print(\"[ERRO] Coleção vazia.\")\n",
        "                    continue\n",
        "                print(\"\\n--- Consulta por Frase ---\")\n",
        "                frase = input(\"Digite a frase exata: \").strip()\n",
        "                frase_processada = sistema.processador.processar(frase)\n",
        "                if not frase_processada:\n",
        "                    print(\"[AVISO] Frase inválida/vazia.\")\n",
        "                    continue\n",
        "                res = sistema.get_indice_invertido_handler().buscar_por_frase(frase_processada)\n",
        "                if res:\n",
        "                    print(f\"✅ Frase encontrada nos documentos: {', '.join(res)}\")\n",
        "                else:\n",
        "                    print(\"❌ Frase não encontrada exatamente nessa ordem.\")\n",
        "            elif choice == '0':\n",
        "                print(\"Encerrando...\")\n",
        "                break\n",
        "            else:\n",
        "                print(\"Opção inválida.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"[ERRO CRÍTICO] {e}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tZmCQEHOgLv1",
        "outputId": "b4b561d6-bb7a-47dc-82af-b14de25f310e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[DEBUG] Baixando recursos do NLTK...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]   Unzipping stemmers/rslp.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[DEBUG] Lendo JSON: colecao - trabalho 01.json\n",
            "[SUCESSO] 50 documentos carregados.\n",
            "\n",
            "=== MENU PRINCIPAL ===\n",
            "1) Adicionar um documento por vez (fila)\n",
            "2) Adicionar TODOS os documentos (lote)\n",
            "3) Remover documento por ID\n",
            "4) Exibir Vocabulário\n",
            "5) Exibir Matriz TF-IDF\n",
            "6) Exibir Índice Invertido (Posicional)\n",
            "7) Consulta Booleana (AND, OR, NOT)\n",
            "8) Consulta por Similaridade (Ranking)\n",
            "9) Consulta por Frase (Exata)\n",
            "0) Sair\n",
            "Escolha uma opção: 1\n",
            "[SUCESSO] Documento D1 adicionado.\n",
            "\n",
            "=== MENU PRINCIPAL ===\n",
            "1) Adicionar um documento por vez (fila)\n",
            "2) Adicionar TODOS os documentos (lote)\n",
            "3) Remover documento por ID\n",
            "4) Exibir Vocabulário\n",
            "5) Exibir Matriz TF-IDF\n",
            "6) Exibir Índice Invertido (Posicional)\n",
            "7) Consulta Booleana (AND, OR, NOT)\n",
            "8) Consulta por Similaridade (Ranking)\n",
            "9) Consulta por Frase (Exata)\n",
            "0) Sair\n",
            "Escolha uma opção: 2\n",
            "Processando inserção em lote...\n",
            "[SUCESSO] 49 documentos adicionados.\n",
            "\n",
            "=== MENU PRINCIPAL ===\n",
            "1) Adicionar um documento por vez (fila)\n",
            "2) Adicionar TODOS os documentos (lote)\n",
            "3) Remover documento por ID\n",
            "4) Exibir Vocabulário\n",
            "5) Exibir Matriz TF-IDF\n",
            "6) Exibir Índice Invertido (Posicional)\n",
            "7) Consulta Booleana (AND, OR, NOT)\n",
            "8) Consulta por Similaridade (Ranking)\n",
            "9) Consulta por Frase (Exata)\n",
            "0) Sair\n"
          ]
        }
      ]
    }
  ]
}